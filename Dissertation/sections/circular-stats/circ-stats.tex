\documentclass[../../ArchStats.tex]{subfiles}

\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}

\begin{document}

% ADD IN UN-CITED TEXTS - STILL A REFERENCE
\nocite{Jammalamadaka2001}

\section{Circular Distributions}
\label{sec:circular-distributions}
\nb{Brief intro sentence to this? Rather abrupt}

The angle between two points of a set is calculated using the inverse tangent of the ratio of the difference in their $y$- and $x$-coordinates. A common implementation is the \textbf{atan2} function, which returns an angle, measured in radians from the positive horizontal axis, that reflects the quadrant in which the point lies, and is defined as
	\begin{equation}
	\label{eqn:atan2}
	\text{atan2} (y, x) = \left\lbrace \begin{matrix*}[l]
	\arctan(y/x), & x > 0 & \, \, \, \, &  \, \, \, \, &\pi/2, & x = 0, y > 0 \\
	\arctan(y/x) + \pi, & x < 0, y \geq 0 & \, \, \, \, &  \, \, \, \, &-\pi/2, & x = 0, y < 0 \\
	\arctan(y/x) - \pi, & x < 0, y < 0 & \, \, \, \, &  \, \, \, \, &\text{undefined}, & x = 0, y = 0 \\
	\end{matrix*} \right. 
	\end{equation}
This choice of angular origin is essentially arbitrary - we could equally easily choose to measure directions against the y-axis, which would displace all of our measurements by $\pi/2$ - and is one of the reasons that directional  data such as these cannot be analysed by simply `flattening' them onto the real line and applying the techniques that we would use for linear distributions. Such an analysis would not reflect the fact that, in circular data, a measurement of $\theta = 1^\circ$ is closer in terms of arc length to $\theta = 359^\circ$ than it is to $\theta = 5^\circ$: the so-called `cross-over' problem \cite{Fisher1993}. This section will provide a brief introduction to statistical methods and models appropriate to circular data. Circular analogues to the measures generally used to describe the shape of data on the real line are described in section \ref{sec:circular-descriptives}; in section \ref{sec:vonMises}, the distribution usually considered analogous to the Normal distribution is given; and a more flexible alternative is discussed in \ref{sec:Jones-Pewsey}.


\subsection{Circular Descriptive Statistics}
\label{sec:circular-descriptives}

Since the angles themselves have no magnitude, a useful formulation in circular statistics, known as the \textit{embedding approach}, is to consider the angle $\theta_j$ as the angle of a unit vector $\mathbf{x}_j$, where $\mathbf{x}_j = (\cos\theta_j, \sin\theta_j)$ are points on a unit circle. This approach allows us to perform certain operations on the angles with results that are invariant or equivariant under rotation - a key requirement in circular inference.

The Moment Generating Function has no equivalent in circular statistics; trigonometric moments can be derived from the unit complex numbers that describe \textbf{x}, although this will not be covered in detail here.

\subsubsection{Location: Circular Mean}
\label{sec:circ-mean}

Due to the `cross-over' problem mentioned previously, we cannot use a simple arithmetic mean to describe the `preferred direction', as we would for data on the real line with a non-arbitrary support; however, we can use the embedding approach already described to find a mean direction using vector addition.

For a set of angles $\theta_1, \dots, \theta_n$ and corresponding unit vectors $\mathbf{x}_1, \dots, \mathbf{x}_n$, the direction of the resultant of $\mathbf{x}_1 + \dots + \mathbf{x}_n$ is the mean direction $\bar{\theta}$. Furthermore, this is the direction of the centre of mass $\mathbf{\bar{x}}$ of ($\mathbf{x}_1, \dots, \mathbf{x}_n$).

Given the Cartesian coordinates $(\cos\theta_j, \sin\theta_j)$ of each $\mathbf{x}_j$, the Cartesian coordinates of $\mathbf{\bar{x}}$ are $(\bar{C}, \bar{S})$, where
	\begin{equation}
	\label{eqn:C-and-S}
	\begin{matrix*}
	\displaystyle{\bar{C} = \frac{1}{n} \sum_{j=1}^{n} \cos \theta_j}, & \, & 
	\displaystyle{\bar{S} = \frac{1}{n} \sum_{j=1}^{n} \sin \theta_j},
	\end{matrix*}
	\end{equation}
and so the direction of the centre of mass from the origin is
	\begin{equation}
	\label{eqn:circ-mean}
	\bar{\theta} = \text{atan2}(\bar{S},\bar{C}).
	\end{equation}
The sample mean thus obtained is equivariant under rotation, in the same way that the sample mean of a data set on the real line is equivariant under translation.


\subsubsection{Concentration and Dispersion}
\label{sec:params-R}
The most commonly used measures of concentration and dispersion also arise from the embedded (vector) approach taken in calculating the sample mean direction. Having used the direction of the mean resultant vector $\mathbf{\bar{x}}$ to obtain $\bar{\theta}$, we have a simple measure of the concentration of the data in its length, 
	\begin{equation}
	\label{eqn:R-bar}
	\bar{R} = \sqrt{(\bar{C}^2 + \bar{S}^2)}.
	\end{equation}
This measure is invariant under rotation and, since all of the vectors $\mathbf{x}_j$ are unit vectors, it must always be the case that $0 \leq \bar{R} \leq 1$, which leads to a straightforward interpretation: if the values are clustered together tightly around the mean direction, then $\bar{R}$ will be close to 1; if they are widely dispersed, $\bar{R}$ will be close to 0. A related measure that is also frequently used - often as an analogue of the variance of data on the real line - is the sample circular variance, $V = 1- \bar{R}$, which also takes values in $[0,1]$. 

It should be remarked that if we were to observe $\bar{R} = 0$, we should not assume that this means that the directions are spread evenly around the circle; for example, a sample consisting of pairs of opposing angles $\theta_1, \dots, \theta_n$ and $\theta_1+\pi, \dots, \theta_n+\pi$ will have $\bar{R} = 0$, but the angles are not necessarily uniformly distributed about the circle. This effect will be observed in any data with a strongly cyclic structure.

Another useful measure of dispersion, used when calculating confidence intervals for the mean direction of the data or comparing the means of multiple samples, is the sample circular dispersion
	\begin{equation}
	\label{eqn:delta-i}
	\hat{\delta} = \frac{1-\bar{R}_2}{2\bar{R}^2},
	\end{equation}
where $\bar{R}_2$ is the mean resultant length of the doubled angles $2\theta_1, \dots, 2\theta_n$. 

Other measures of the dispersion of the data are sometimes used, including an analogue to the  sample standard deviation, which can be useful when comparing angular data to a distribution on the real line: $\hat{\sigma} = \left\lbrace -2 \log \bar{R} \right\rbrace ^{1/2} \in [0, \infty]$. However, given that the circular distributions with which we will be working have finite support $[0, 2\pi)$, the finite-valued measures $\bar{R}$ and $V$ are generally a more natural choice here. In particular, because of the close relationship between $\bar{R}$ and its population analogue $\rho$ to the parameter $\kappa$ of the circular distributions what will be introduced in subsections \ref{sec:vonMises} and \ref{sec:Jones-Pewsey}, $\bar{R}$ will be used as the main measure of dispersion in this report.
 
%Mention Batschelet?


\subsubsection{Shape: Skewness and Kurtosis}
\label{sec:shape}

The second central sine moment, $\bar{b}_2$, may be used as a measure of the skewness of the data about the mean direction, with
	\begin{equation}
	\label{eqn:bar-b-2}
	\bar{b}_2 = \frac{1}{n} \sum_{j=1}^n \sin 2(\theta_j-\bar{\theta}) = \bar{R}_2 \sin(\bar{\theta}_2 - 2\bar{\theta})
	\end{equation}
where $\bar{R}_2$ is the mean resultant length of the doubled angles, and $\bar{\theta}_2$ their mean direction. 

Values of $\bar{b}_2$ close to 0 indicate data that is near-symmetric, with larger  absolute values indicating data that is skewed away from the mean: in a clockwise direction for positive values, and anti-clockwise for negative, with $\pm1$ indicating maximum skewness.

A similar measure of the sample kurtosis is given by the second central cosine moment, $\bar{a}_2$:
	\begin{equation}
 	\bar{a}_2 = \frac{1}{n} \sum_{j=1}^n \cos 2(\theta_j-\bar{\theta}) = \bar{R}_2 \cos(\bar{\theta}_2 - 2\bar{\theta}) 
 	\end{equation}
 with $\bar{a}_2 - \bar{R}^4$ more usually used to assess the degree of kurtosis than $\bar{a}_2$ alone. 
Values close to 0 indicate near-even distribution of the data, while values close to 1 indicate extremely peaked data, with all angles almost identical. A measure that is often easier to interpret is the excess kurtosis, $\bar{a}_2 - \bar{R}^4$, which adjusts the raw kurtosis score $\bar{a}_2$ by the degree of kurtosis $\bar{R}^4$ we would expect to see if a normal distribution was wrapped onto the unit circle. Thus, if $\bar{a}_2 - \bar{R}^4 > 0$, the data is more peaked than a wrapped normal distribution, while if $\bar{a}_2 - \bar{R}^4 < 0$, the excess kurtosis is negative, and we expect to see a distribution that is flatter than a wrapped normal distribution.

Mardia \cite{Mardia1972} proposed further standardized measures of skewness and kurtosis, which can result in much higher absolute values, particularly when the data is very concentrated. However, our primary interest is not in the exact degree of skew or kurtosis - it is sufficient for us to ascertain whether or not either attribute might plausibly be zero, so we will continue to use the simpler measures.

\subsubsection{Estimation of population parameters}
\label{sec:bias-corrected}
The sample summary statistics $\bar{\theta}$, $\bar{R}$, $\bar{b}_2$, and $\bar{a}_2$ have as their population analogues $\mu$, $\rho$, $\bar{\beta}_2$ and $\bar{\alpha}_2$ respectively. However, it has been shown \cite{Pewsey2004b} that these estimators are biased, with biases and sampling distributions depending on the size of $n, \rho$, and the second, third and fourth central trigonometric moments of the sample. Following Pewsey \cite{Pewsey2014}, the following bias-corrected estimators will be used when describing the underlying distribution of a sample:
	\begin{eqnarray}
	\hat{\mu}_{BC} &=& \bar{\theta} + \left(\frac{\bar{b}_2}{2n\bar{R}^2} \right)\\[5pt]
	\hat{\rho}_{BC} &=& \bar{R} - \left(\frac{1-\bar{a}_2}{4n\bar{R}}\right)\\[5pt]
	\widehat{\bar{\beta}_2}_{BC} &=& \bar{b}_2 - \frac{1}{n\bar{R}} \left(-\bar{b}_3 - \frac{\bar{b}_2}{\bar{R}} + \frac{2\bar{a}_2\bar{b}_2}{\bar{R}^3}\right)\\[5pt]
	\widehat{\bar{\alpha}_2}_{BC} &=& \bar{a}_2 - \frac{1}{n} \left(1-\frac{\bar{a}_3}{\bar{R}}-\frac{\bar{a}_2(1-\bar{a}_2) + \bar{b}_2^2}{\bar{R}^2}\right)
	\end{eqnarray}

Where the sample is of sufficient size, nominal $100(1-\alpha)\%$ confidence intervals can be obtained for any population parameter $\hat{\zeta}_{BC}$ using $z_{1-\alpha/2}$, the $(1-\alpha/2)$-quantile of $N(0,1)$, and the variance of the population analogue of $\zeta$ - here denoted $\bar{\zeta}$ - to find
\begin{equation}
\bar{\zeta}_{BC} \pm \sqrt{z_{1-\alpha/2} \widehat{\text{var}}(\bar{\zeta})}. \end{equation}
Estimates of the variances of the sample values are given by
	\begin{eqnarray}
	\widehat{\text{var}}(\bar{\theta}) &=& \frac{1-\bar{a}_2}{2n\bar{R}^2} \\[5pt]
	\widehat{\text{var}}(\bar{R}) &=& \frac{1-2\bar{R}^2 + \bar{a}_2}{2n} \\[5pt]
	\label{eqn:var-b-2}
	\widehat{\text{var}}(\bar{b}_2) &=& \frac{1}{n}\left[ \frac{1-\bar{a}_4}{2} - 2\bar{a}_2 - \bar{b}_2^2 + \frac{2\bar{a}_2}{\bar{R}} \left\lbrace \bar{a}_3 + \frac{\bar{a}_2 (1-\bar{a}_2)}{\bar{R}} \right\rbrace \right] \\[5pt]
	\widehat{\text{var}}(\bar{a}_2) &=& \frac{1}{n}\left[ \frac{1-2\bar{a}_2^2 + \bar{a}_4}{2} + \frac{2\bar{b}_2}{\bar{R}} \left\lbrace \bar{b}_3 + \frac{\bar{b}_2 (1-\bar{a}_2)}{\bar{R}} \right\rbrace \right]
	\end{eqnarray}
In each case, $\bar{b}_p$ and $\bar{a}_p$ are the $p$th central sine and cosine moments, obtained by 
	\begin{equation}
	\bar{a}_p = \frac{1}{n} \sum_{j=1}^n \cos p(\theta_j-\bar{\theta}), \, \, \, 
\bar{b}_p = \frac{1}{n} \sum_{j=1}^n \sin p(\theta_j-\bar{\theta}).
	\end{equation}

Where the sample size is smaller, a bootstrapped confidence interval can be computed. The bias-corrected parameter estimate $\hat{\zeta}_{BC}$ is computed for the original data and for each of $B$ bootstrap samples; the resulting $B+1$ estimates are ordered, and the $\alpha/2$ and $1- \alpha_2$ quantiles used as the bounds for the confidence interval.


%=======================================================================================

\subsection{The von Mises or `Circular Normal' Distribution}
\label{sec:vonMises}

When considering the distribution of angles between points that are oriented approximately along a single axis, we might expect to see something that resembles a normal distribution wrapped onto the unit circle. The wrapped normal distribution thus envisaged, however is rather complicated, and unlike the standard normal distribution, does not belong to the exponential family, making inference on the distribution difficult.

A more tractable choice - and one that does belong to the exponential family - is the von Mises distribution. Originally proposed in 1918 to model the deviations from integer values of molecular weights \cite{VonMises1918}, the von Mises distribution is the most commonly used circular distribution, so much so that it is also referred to as the circular normal distribution. For sufficiently concentrated data, the von Mises distribution can be used to approximate not only the wrapped normal distribution but also the normal distribution on the real line, making it a natural candidate model for the post-hole data.

\subsubsection{Density}
The von Mises distribution $vM(\mu, \kappa)$ (sometimes also written $M(\mu, \kappa)$ or, reflecting its alternative name of the circular normal distribution, $CN(\mu, \kappa)$) has two parameters, the location $\mu$ and concentration parameter $\kappa$. The probability density function is
	\begin{equation}
	\label{eq:vM-density}
	f(\theta; \mu, \kappa) = \frac{e^{\kappa \cos(\theta - \mu)}}{2\pi I_0(\kappa)},	
	\end{equation}
with $I_0(\kappa)$ the modified Bessel function of the first kind and order $p=0$, where
	\begin{equation}
	\label{eq:mod-Bessel}
	I_p(\kappa) = \frac{1}{2\pi}\int_0^{2\pi} \cos(p\theta)e^{\kappa \cos \theta} d\theta.
	\end{equation}
The Bessel function cannot be evaluated directly, so numerical integration is required to calculate the value of the normalising constant. Being reflectively symmetric about $\mu$, the distribution will have skewness 0 and, since it closely approximates the wrapped normal distribution, will have excess kurtosis close to 0. 

The concentration parameter $\kappa$ is related to the mean resultant length $\rho$ through the definition of $\rho$ as the first trigonometric moment: $\rho = A_1(\kappa)$, where
	\begin{equation}
	A_p(\kappa) = I_p(\kappa)/I_0(\kappa)
	\end{equation}
and $I_p(\kappa)$ is as defined above.

Although negative values of $\kappa$ are admissable, the convention is to take $\kappa > 0$, since $vM(\mu, \kappa)$ and $vM(\mu + \pi, -\kappa)$ give the same distribution of $\theta$. This also lends itself to a simple, intuitive interpretation of the concentration parameter: when $\kappa = 0$, the distribution of $\theta$ is uniform about the circle, growing more concentrated about $\mu$ as $\kappa$ increases.  The ratio of the density at the mode to the density at the antimode is $f(\mu) / f(\mu + \pi) = e^{2\kappa}$; for values of $\kappa$ greater than around 2, the density at the antipode is essentially negligible, allowing approximation to the normal distribution. Figure \ref{fig:von-Mises-densities} shows the effect of varying $\kappa$ on the von Mises density. A linear plot is given alongside the polar plot for easier visual comparison of the two distributions; the support of the density is essentially arbitrary, so a cut-point of $\mu \pm \pi$ has been used to centre the peak of the data along the $x$-axis.
 
\begin{figure}[!h]
\centering
\caption{von Mises densities with $\mu = \pi$ and varying $\kappa$.}
\label{fig:von-Mises-densities}
%
\begin{subfigure}[t]{0.4\textwidth}
\centering
\caption{Circular plot of $vM(\pi, \kappa)$}
\includegraphics[scale=0.35]{./img/vM-circ-plot.pdf}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
\centering
\caption{Linear plot of $vM(\pi, \kappa)$}
\includegraphics[scale=0.3]{./img/vM-linear-plot.pdf}
\end{subfigure}
%
\end{figure}

%The second central trigonometric moments are $\bar{\beta}_2 = A_2(\kappa) \sin(2\mu)$ and $\bar{\alpha}_2 = A_2(\kappa) \cos(2\mu)$, where $A_2(\kappa)$ is as defined above; it can be shown that $\bar{\beta}_2 = 0$ - as we would expect, given the distribution's reflective symmetry - and that $\bar{\alpha}_2 = 1 - 2A_1(\kappa)/\kappa = 1-2\rho/\kappa$. \nb{Find more to say on this - or just take it out}

The cumulative distribution function, which will be required to test the goodness of fit of the model, is
	\begin{equation}
	F(\theta; \mu, \kappa) = \frac{1}{2\pi I_0(\kappa)} \int_0^\theta e^{\kappa \cos(\phi - \mu)}d\phi
	\end{equation}
for $0 \leq \theta < 2\pi$; again, numerical evaluation of this expression is necessary.
 
%Since we cannot guarantee that this is the case - and in fact, we might hope that it is not the case \nb{check simulations to see if this is so!} - we should also consider a more general distribution that is better able to handle data with non-zero kurtosis.
%\nb{is kurtosis related to $\psi \kappa$ in any way?}


\subsubsection{Parameter Estimation}
\nb{Is this notation confusing? Using hat for BC and for MLE? Maybe use a tilde instead. Or be more rigourous with the BC subscript}

The parameters of the generating von Mises distribution are usually estimated using maximum likelihood methods. As in the case of the normal distribution, the maximum likelihood estimator $\hat{\mu}$ is the sample mean $\bar{\theta}$, and is unbiased.

The MLE $\hat{\kappa}$ for the concentration parameter can be obtained using $\bar{R}$ as an estimator for $\rho$ \nb{bias corrected?}, giving $\hat{\kappa} = A_1^{-1}(\bar{R})$. \nb{Define A1inv} Although asymptotically unbiased, $\hat{\kappa}$ is a biased estimator, tending to substantially over-estimate $\kappa$ in small samples and for dispersed data, particularly when $\bar{R}$ is less than 0.7. For samples with $n < 16$, we will use Fisher's bias-corrected estimate \cite{Fisher1993}:
	\begin{equation}
	\hat{\kappa} = \left\lbrace \begin{matrix*}[l]
	\text{max} ( \hat{\kappa}_{ML} - 2(n \hat{\kappa}_{ML})^{-1}, 0) & & \hat{\kappa}_{ML} < 2 \\
	(n-1)^3 \hat{\kappa}_{ML} / (n^3 + n) & &  \hat{\kappa}_{ML} \geq 2
	\end{matrix*} \right. 
	\end{equation}
Even with this adjustment, Fisher warns that we can expect to have difficulty in fitting a von Mises distribution to dispersed data -  which he defines as any data with $\kappa < 0.7$, and particularly with $\kappa < 0.45$. Where the data is dispersed to this degree, but still demonstrably unimodal - or where the excess kurtosis of the data is greater than 0 - an alternative model should be considered.

%=======================================================================================


\subsection{A generalisation: the Jones-Pewsey distribution}
\label{sec:Jones-Pewsey}
A more flexible distribution that is able to account for non-zero kurtosis is the three-parameter Jones-Pewsey distribution, first proposed in \cite{Jones2005}, and expanded in \cite{Pewsey 2014}.

\subsubsection{Density}
The Jones-Pewsey model $JP(\mu, \kappa, \psi)$ has density
	\begin{equation}
	f(\theta) = \frac{\left\lbrace \cosh(\kappa\psi) + \sinh(\kappa\psi) \cos(\theta - \mu) \right\rbrace ^ {1/\psi}}{2\pi P_{1/\psi}(\cosh(\kappa\psi))}
	\end{equation}
The normalising constant here is an associated Legendre function of the first kind of degree $1/\psi$ and order 0:
	\begin{equation}
	2\pi P_{1/\psi}(\cosh(\kappa\psi)) = \int_{-\pi}^\pi \left( \cosh(\kappa \psi) + \sinh(\kappa\psi)\cos(\theta)\right)^{1/\psi}d\theta.
	\end{equation}
As in the case of the Bessel function used in the von Mises distribution, the normalising constant must be evaluated numerically. The distribution function has no general closed form.

As in the von Mises model, $\mu$ is the mean direction of the distribution, while $\kappa$ reflects the degree of concentration of the angles about the mean direction. However, here the degree of concentration is modified by the shape parameter, $\psi \in (-\infty, \infty)$. Negative values of $\psi$ indicate a more peaked distribution; a distribution with $\psi \leq 0$ have more of its density concentrated about $\mu$, and lighter shoulders, than a distribution with the same $\kappa$ and $\psi \geq 0$, with the effect increasing as $\psi$ increases in absolute value. For distributions with $\psi > 0$, there is relatively little difference between the densities of distributions with different values of $\kappa$. Figure \ref{fig:JP-densities} shows the behaviour of the Jones-Pewsey distribution when $\kappa$ and $\mu$ are fixed and $\psi$ is varied. Even with a relatively low value of $\kappa$, when $\psi = -2$ the density is extremely concentrated around the mean - to show the full peak here, the $y$-axis of the plot would need to run from 0 to 2.6.

\begin{figure}[!h]
\centering
\caption{Jones-Pewsey densities with $\mu = \pi$, $\kappa = 2$, and varying $\psi$}
\label{fig:JP-densities}
%
\begin{subfigure}[t]{0.4\textwidth}
\centering
\caption{Circular plot of $JP(\pi, 2, \psi)$}
\includegraphics[scale=0.35]{./img/JP-circ-plot.pdf}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
\centering
\caption{Linear plot of $JP(\pi, 2, \psi)$}
\includegraphics[scale=0.3]{./img/JP-linear-plot.pdf}
\end{subfigure}
%
\end{figure}

Copmpared to a von Mises distribution with the same value of $\kappa$, the Jones-Pewsey model with $\psi < 0$ distributes more of the its mass around the antipode, with the difference becoming more pronounced as $\kappa$ increases. We might therefore expect a Jones-Pewsey model to better fit the data where we see a region of high concentration above a reasonably uniform distribution of angles, or where the data is otherwise particularly peaked.

\subsubsection{Special cases}

The Jones-Pewsey distribution includes as special or limiting cases a number of the classical circular distributions, specified by the value of the shape parameter $\psi$. This means that we are able to fit a single Jones-Pewsey model to our data, and assess plausible values of $\psi$ to determine whether a simpler model might be more appropriate given the data, without having to fit and compare each model sequentially. When $\psi = -1$, the density simplifies to that of a Wrapped Cauchy distribution; for $\psi = 1$, the Cardioid; and for $\psi > 0, \kappa \rightarrow \infty$, we have Cartwright's power-of-cosine distribution. Since these latter limiting cases did not arise during the project, their distributions will  not be described in detail here; for a thorough treatment, see \cite{Jones2005}. 

For small $\vert \psi \vert$, $\left\lbrace \cosh(\kappa\psi) + \sinh(\kappa\psi) \cos(\theta) \right\rbrace \simeq e^{\kappa \cos \theta}$ and the density kernel of the Jones-Pewsey is approximately equal to the kernel of the von Mises distribution; moreover, it can be shown that $\Lim{\psi \rightarrow 0} P_{1/\psi}(\cosh(\kappa\psi)) = I_0(\kappa)$, and so as $\psi \rightarrow 0$, we obtain the von Mises distribution. For $\kappa = 0$, we have the continuous circular uniform distribution; this is true for all values of $\psi$ and $\mu$.

For $-\infty < \psi < -2$ and $\kappa \rightarrow \infty$, the Jones-Pewsey distribution describes a density not found in any of the classical models, with a pole at 0. It can be shown that, for $-\infty < \psi < -2$ and $\kappa \rightarrow \infty$, the density is that of $\theta = 2 \cos^{-1}(B)$, where $B \sim \text{Beta}(\alpha, \beta)$ rescaled to  $[-1,1]$, with $\alpha = \beta = (1/\psi) + \frac{1}{2}$.


\subsubsection{Parameter estimation}

Generally, closed-form expressions for the parameters of the Jones-Pewsey distribution are not available, so in practice, maximum likelihood estimation is carried out by numerical optimization of the log-likelihood function. For larger samples, confidence intervals can be calculated using asymptotic normal theory; inverting the Hessian matrix obtained during optimization of the likelihood function gives the observed Fisher information matrix, the square roots of the diagonal of which provide the asymptotic standard errors of the maximum likelihood estimates. A nominal confidence interval can then be obtained by multiplying the standard error by the appropriate quantile of the standard normal distribution.

Alternatively, particularly for smaller samples, confidence intervals may be obtained using a bootstrap method. Here, the maximum likelihood estimates of the parameters are obtained for the original data set; $B$ random samples of the same size as the original sample are drawn from the model thus defined, and their parameters estimated; the estimates are then ordered, and appropriate quantiles found.

Some instabilities in the numerical approximation can arise when $\vert \kappa \psi \vert$ is large, so all calculations should be limited to the case when $\vert \kappa \psi \vert < 10$. However, in practice we have found that the values of $\kappa \psi$ obtained from the data do not often approach this limit. Large negative values of $\kappa \psi$ would indicate something approaching a point process in which all of the points are lying on a straight line, while large positive values represent flat, cardioid-like distributions, which are too close to uniformity to be of any use in identifying gridding.

\end{document}